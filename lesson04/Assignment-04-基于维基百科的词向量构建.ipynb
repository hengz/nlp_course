{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-04 基于维基百科的词向量构建\n",
    "\n",
    "在本章，你将使用Gensim和维基百科获得你的第一批词向量，并且感受词向量的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-01: Download Wikipedia Chinese Corpus\n",
    "\n",
    "第一步：使用维基百科下载中文语料库\n",
    "\n",
    "https://dumps.wikimedia.org/zhwiki/20190720/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-28T15:26:48.029700Z",
     "start_time": "2019-07-28T15:26:48.025673Z"
    }
   },
   "source": [
    "## Step-02: Using wikiextractor to extract the wikipedia corpus\n",
    "\n",
    "第二步：使用python wikipedia extractor抽取维基百科的内容\n",
    "\n",
    "https://github.com/attardi/wikiextractor\n",
    "\n",
    "执行：\n",
    "\n",
    "```shell\n",
    "> python WikiExtractor.py -o .\\output D:\\BaiduYunDownload\\维基百科中文20190720\\zhwiki-20190720-pages-articles-multistream.xml.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step-03: Using gensim get word vectors:\n",
    "Reference:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "\n",
    "第三步：参考Gensim的文档和Kaggle的参考文档，获得词向量。 注意，你要使用Jieba分词把维基百科的内容切分成一个一个单词，然后存进新的文件中。然后，你需要用Gensim的LineSentence这个类进行文件的读取。\n",
    "\n",
    "在训练成词向量Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cut words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:26:27.616295Z",
     "start_time": "2019-08-15T08:26:27.614310Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jieba.analyse\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "corpus_path = 'E:\\\\GitHub\\\\wikiextractor\\\\output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:26:05.901271Z",
     "start_time": "2019-08-15T08:26:05.896275Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_files(root_path):\n",
    "    \"\"\"\n",
    "    return all file pathes as a list under one directory\n",
    "    \"\"\"\n",
    "    pathes = []\n",
    "    for root, dirs, files in os.walk(corpus_path):\n",
    "        if not files:\n",
    "            continue\n",
    "        for file in files:\n",
    "            pathes.append(root + '\\\\' + file)\n",
    "    return pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:49:57.546060Z",
     "start_time": "2019-08-15T08:49:47.025133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "e:\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    preprocess text, drop number, blank, stopwords\n",
    "    return segments list\n",
    "    \"\"\"\n",
    "    stopwords=pd.read_csv('.//stopwords.txt',index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "    stopwords=stopwords['stopword'].values\n",
    "    \n",
    "    try:\n",
    "        segs = list(jieba.cut(text))\n",
    "        segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "        segs = list(filter(lambda x:x.strip(), segs)) #去左右空格\n",
    "        segs = list(filter(lambda x:len(x)>1, segs))#长度为1的字符\n",
    "        segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:50:40.709137Z",
     "start_time": "2019-08-15T08:50:25.070173Z"
    }
   },
   "outputs": [],
   "source": [
    "file_pathes = get_all_files(corpus_path)\n",
    "sentences_path = 'E:\\\\corpus'\n",
    "\n",
    "limit = 100\n",
    "i = 0\n",
    "\n",
    "for file_path in file_pathes:\n",
    "    with open(file_path, 'r', encoding='utf-8') as rf:\n",
    "        with open(sentences_path+'\\\\'+str(i)+'.txt', 'w+', encoding='utf-8') as wf:\n",
    "            for line in rf.readlines():\n",
    "                if line == '\\n':\n",
    "                    continue\n",
    "                if line[0] == '<':\n",
    "                    continue\n",
    "                i += 1\n",
    "                segs = preprocess_text(line)\n",
    "                wf.write(' '.join(segs))\n",
    "    if i > limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:59:01.224502Z",
     "start_time": "2019-08-15T08:59:01.205513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "words_files = os.listdir(sentences_path) \n",
    "for words_file in words_files:\n",
    "    words_file_dir = sentences_path + '\\\\' + words_file\n",
    "    sentences = LineSentence(words_file_dir)\n",
    "\n",
    "    '''\n",
    "    LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。\n",
    "    size：是每个词的向量维度； \n",
    "    window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； \n",
    "    min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； \n",
    "    workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。\n",
    "    sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW\n",
    "    alpha (float, optional) – 初始学习率\n",
    "    iter (int, optional) – 迭代次数，默认为5\n",
    "    '''\n",
    "    model = Word2Vec(sentences=sentences, size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus('/text8')\n",
    "model = word2vec.Word2Vec(sentences, sg=1, size=100,  window=5,  min_count=5,  negative=3, sample=0.001, hs=1, workers=4)\n",
    "model.save('/text82.model')\n",
    "print(model['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-04: Using some words to test your preformance.\n",
    "\n",
    "第四步，测试同义词，找几个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-05: Using visualization tools\n",
    "\n",
    "https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "\n",
    "第五步：使用Kaggle给出的T-SEN进行词向量的可视化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
